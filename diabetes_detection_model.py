# -*- coding: utf-8 -*-
"""diabetes_detection_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1incjRsWJQfcXb5pq6S83g6iZRywbNRzg
"""

import zipfile
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

#parameters
batch_size=25
epochs=500
shuffle=True
size_of_test_set=0.2
lr=0.001
classification_threshold=0.35
momentum_val=0.8
dropout_val=0.2


with zipfile.ZipFile("diab.zip","r") as zip_ref:
  zip_ref.extractall("")

#getting the data
data=pd.read_csv("diabetes.csv")

#first five entries in dataframe
print(data.head())

#information about data such as null values and statistical data
data.info()
data.describe()

#features and labels
y=data["Outcome"]
x=data.drop("Outcome",axis=1)

#determine null values or missing values
age=x["Age"]
preg=x["Pregnancies"]
x=x[["Glucose","BloodPressure","SkinThickness","Insulin","BMI","DiabetesPedigreeFunction"]].replace(0,np.nan)

x.head()
x.info()
x.describe()

#imputer to impute mean value for each feature
imputer=SimpleImputer(strategy="mean")
imputer.fit(x)
print(imputer.statistics_)

#transforming feature data
X=imputer.transform(x)
x_transf=pd.DataFrame(X,columns=x.columns,index=x.index)
x_transf["Pregnancies"]=preg
x_transf["Age"]=age

#scaling values using minmaxscaler in range 0 to 1
scale=MinMaxScaler()
x_transf[x_transf.columns] = pd.DataFrame(scale.fit_transform(x_transf[x_transf.columns].values), columns=x_transf.columns, index=x_transf.index)
x_transf.head()

#creating a copy to obtain correlation after transformation
x_temp=x_transf.copy()
x_temp["Outcome"]=y
x_temp.head()
corr_matrix=x_temp.corr()
print(corr_matrix["Outcome"].sort_values(ascending=False))

#split data into training and testing set, maintaining same ratio of outcome
x_train,x_test=train_test_split(x_transf,test_size=size_of_test_set,random_state=42,shuffle=shuffle)
y_train,y_test=train_test_split(y,test_size=size_of_test_set,random_state=42,shuffle=shuffle)

#outcome has same ratio for 1's and 0's
print(y_train.value_counts())
print(y_test.value_counts())

#callback function to stop training once recall reaches 88%
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self,epochs,logs={}):
    if logs.get("recall")>=0.88 and logs.get("auc")>=0.80:
      print("Reached recall of 88%")
      self.model.stop_training=True
callback=myCallback()


#metrics for measuring accuracy
my_metrics = [
      tf.keras.metrics.BinaryAccuracy(name='accuracy',threshold=classification_threshold),
      tf.keras.metrics.Precision(name='precision',thresholds=classification_threshold),
      tf.keras.metrics.Recall(name="recall",thresholds=classification_threshold),
      tf.keras.metrics.AUC(num_thresholds=150, name='auc'),
]

#creating model
model=tf.keras.Sequential([
                           tf.keras.layers.Dense(16,input_shape=[len(x_train.keys())],activation="relu"),
                           tf.keras.layers.Dropout(dropout_val),
                           tf.keras.layers.Dense(16,activation="relu"),
                           tf.keras.layers.Dense(8,activation="relu"),
                           tf.keras.layers.Dense(1,activation="sigmoid")
])

#compiling the model
model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr,momentum=momentum_val),
                loss=tf.keras.losses.BinaryCrossentropy(),
                metrics=my_metrics)

#summary of layers involved in model created
model.summary()

#training the model
history = model.fit(x=x_train, y=y_train, batch_size=batch_size,epochs=epochs, shuffle=shuffle,callbacks=[callback])

#save model to load it later
model.save("my_model.h5")

#evaluate model against unseen data i.e. test set
model.evaluate(x = x_test, y = y_test, batch_size=batch_size)

epochs = history.epoch
hist = pd.DataFrame(history.history)

def plot_curve(epochs, hist, list_of_metrics,name_of_plot,legend_pos):
  plt.figure()
  plt.xlabel("Epoch")
  plt.ylabel("Value")
  plt.title(name_of_plot)

  plt.tick_params(left=False,bottom=False)
  for pos in ['right', 'top']:
      plt.gca().spines[pos].set_visible(False)
  for m in list_of_metrics:
    x = hist[m]
    plt.plot(epochs[1:], x[1:], label=m)
  plt.legend(loc=legend_pos)
  plt.savefig("{}.png".format(name_of_plot),dpi=300,bbox_inches="tight")
  plt.show()

#graph for accuracy, precision and recall, recall being important parameter for diabetes detection
list_of_metrics_to_plot = ['accuracy', "precision", "recall"]
plot_curve(epochs, hist, list_of_metrics_to_plot,"Accuracy, precision and recall","lower right")

#auc of roc
list_of_metrics_to_plot = ['auc']
plot_curve(epochs,hist,list_of_metrics_to_plot,"AUC","lower right")

list_of_metrics_to_plot=["accuracy"]
plot_curve(epochs,hist,list_of_metrics_to_plot,"Models's accuracy","lower right")

list_of_metrics_to_plot=["loss"]
plot_curve(epochs,hist,list_of_metrics_to_plot,"Model's loss","upper right")
